<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hey it's violet</title>
  <subtitle>developer &amp; human in the PNW</subtitle>
  <id>https://vgpena.github.io/</id>
  <link href="https://vgpena.github.io"/>
  <link href="https://vgpena.github.io/feed.xml" rel="self"/>
  <updated>2017-09-02T19:05:00-07:00</updated>
  <author>
    <name>Violet Pe√±a</name>
  </author>
  <entry>
    <title>Classifying Tweets with Keras and TensorFlow</title>
    <link rel="alternate" href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/"/>
    <id>https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/</id>
    <published>2017-09-02T19:05:00-07:00</published>
    <updated>2017-09-04T20:01:53-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">I had a week to make my first neural network. I dove into TensorFlow and Keras, and came out with a deep neural network, trained on tweets, that can classify text sentiment. Here's an introduction to neural networks and machine learning, and step-by-step instructions of how to do it yourself.</summary>
    <content type="html">&lt;p&gt;Summer is drawing to a close. The air is humid and still. You‚Äôre between projects at work. What do you do with these few empty days?&lt;/p&gt;

&lt;p&gt;If you‚Äôre like me, you train a neural net. It had been on my ‚ÄúTo Do‚Äù list for about a year now, and while I had done some reading and tutorials, I hadn‚Äôt yet made my own from the ground up.&lt;/p&gt;

&lt;p&gt;I spent a few days chasing dead-ish ends, doing even more tutorials, and tinkering. I emerged with a custom neural net that could classify text as positive or negative with 79.3% accuracy.&lt;/p&gt;

&lt;p&gt;Here‚Äôs how to create your own neural net using Python, TensorFlow, and Keras! Happy learnings ‚ú®&lt;/p&gt;
&lt;h2 class='section-title' id=so-what-do-neural-nets-do&gt;&lt;a href='#so-what-do-neural-nets-do' class='section-inner'&gt;So what do neural nets do?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Neural net[work]s are collections of nodes that apply transformations to data. Their core behavior is: given an input, generate an output.&lt;/p&gt;

&lt;p&gt;The intriguing aspect of neural nets is that we don‚Äôt tell them &lt;em&gt;how&lt;/em&gt; to generate that output. Rather, we set them up to ‚Äúlearn‚Äù how to generate outputs based on massive amounts of training data. Training data consists of an input and an output, usually labelled by humans. The neural net intakes a piece of training data, generates an output, compares that output to the actual result, and adjusts the weights on its nodes ‚Äî that is, how likely an individual node is to return a certain intermediate value. Modifying these weights leads a net to return one value or another given an input, and is how the net refines its accuracy as it trains.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/nnet.png" alt="A diagram of a neural network with two hidden layers in addition to input and output layers." title="A sample neural network." width="597px" height="324px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A neural network with two hidden layers. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Nodes are arranged in layers within the neural net. All neural nets have an input layer and an output layer; within, they have at least one additional ‚Äúhidden‚Äù layer. ‚ÄúDeep‚Äù neural nets have more than one hidden layer. This is also the difference, name-wise, between ‚Äúmachine‚Äù and ‚Äúdeep‚Äù learning.
Once you train a neural net, it contains a fairly accurate self-adjusted system for creating outputs. Ideally, you can feed novel data into the net and end up with a meaningful output.&lt;/p&gt;

&lt;p&gt;Neural nets can either &lt;em&gt;classify&lt;/em&gt; extant data or &lt;em&gt;predict&lt;/em&gt; new data. &lt;a href="https://www.technologyreview.com/s/419223/using-neural-networks-to-classify-music/"&gt;Identifying musical genre&lt;/a&gt; is an example of the former; &lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html"&gt;fusing visual styles&lt;/a&gt;, of the latter. Kristen Stewart -- yes, the lead from &lt;em&gt;Twilight&lt;/em&gt; -- used this to give her film &lt;em&gt;Come Swim&lt;/em&gt; (2017) an impressionist look. She even &lt;a href="https://arxiv.org/pdf/1701.04928v1.pdf"&gt;co-authored a paper&lt;/a&gt; about the technique üî•üî•üî•&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-primary"&gt;
    &lt;img src="/images/keras/come-swim.png" alt="Four squares illustrating different degrees of visual treatment for &amp;quot;Come Swim&amp;quot;. All depict a man in water, but each has been modified to have its own visual style." title="Details from &amp;quot;Come Swim&amp;quot;" width="1404px" height="797px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Fine-tuning the visual treatment for &lt;em&gt;Come Swim&lt;/em&gt;. &lt;a href="https://arxiv.org/pdf/1701.04928v1.pdf"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Deep Dream&lt;/a&gt;, a project out of Google, both classifies and predicts. It uses classifications to suggest predictions, which in turn amplify the certainty of classifications. It turns landscapes and portraits into fields of roiling curves often resembling human eyes.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/deep-dream.jpg" alt="&amp;quot;The Scream&amp;quot; by Edvard Munch, but all the fields of color have been subdivided into roiling curls resembling nested eyes, cars, and dogs." title="A sample Deep Dream project" width="767px" height="965px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A sample of what Deep Dream can do when applied to Edvard Munch&amp;#39;s &lt;em&gt;The Scream&lt;/em&gt;. &lt;a href="https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ/photo/AF1QipPAcXwHkI3k8Sqnq-3WJUfXCZR68rZYTiR2b3te?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I won‚Äôt get into the specifics of neural nets or training/adjustment mechanics here. I‚Äôve done some reading but I have a long way to go before I am truly 1337. I highly recommend &lt;em&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/em&gt; if you‚Äôre looking for a great intro to the field.&lt;/p&gt;
&lt;h2 class='section-title' id=whats-emourem-neural-net-going-to-do&gt;&lt;a href='#whats-emourem-neural-net-going-to-do' class='section-inner'&gt;What‚Äôs &lt;em&gt;our&lt;/em&gt; neural net going to do?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We‚Äôll perform the relatively straightforward task of classifying text ‚Äî specifically, we‚Äôll predict whether text expresses a positive or a negative sentiment. As training data, we‚Äôre using the behemoth &lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;Twitter Sentiment Analysis Dataset&lt;/a&gt; documented at ThinkNook.&lt;/p&gt;
&lt;h2 class='section-title' id=getting-started-environment-and-tools&gt;&lt;a href='#getting-started-environment-and-tools' class='section-inner'&gt;Getting started: environment and tools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Our neural net is Python 2 based, so make sure that‚Äôs what you‚Äôre working with on your own machine. I highly recommend &lt;a href="https://virtualenv.pypa.io/en/stable/"&gt;virtualenv&lt;/a&gt; for managing your package installs, and &lt;a href="https://ipython.org/"&gt;IPython&lt;/a&gt; as an interactive editor.
The net itself will be built using &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt;, an open-source, Google-backed machine learning framework. We‚Äôre laying &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; on top of TensorFlow to act as an API and simplify TensorFlow‚Äôs syntax.
If you want to dig into TensorFlow on its own for a bit, &lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;their ‚ÄúFor Beginners‚Äù tutorial&lt;/a&gt; is informative and surprisingly painless.&lt;/p&gt;
&lt;h2 class='section-title' id=language-and-machines&gt;&lt;a href='#language-and-machines' class='section-inner'&gt;Language and machines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For me, there are few joys on par with working with natural language. In machine learning, we have two ways of representing language language: vector embeddings or one-hot matrices.
Vector embeddings are spatial mappings of words or phrases. Relative locations of words indicate similarity and suggest semantic relationships ‚Äî for instance, &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;vector embeddings can be used to generate analogies&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/vectors.png" alt="Graphs depict vector embeddings -- circles with lines drawn between them to convey analogous relationships between the words represented by the circles." title="A sample neural network." width="1576px" height="772px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Sample vector embeddings that demonstrate analogous relationships between words. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;One-hot matrices, on the other hand, contain no linguistic information. (If you‚Äôre taking all the recommended detours, you‚Äôll remember one-hot matrices from the &lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;TensorFlow MNIST digit classification tutorial&lt;/a&gt;.) They‚Äôre na√Øve; they indicate what data they contain but suggest nothing &lt;em&gt;about&lt;/em&gt; that data, or its relationship to other information.
I decided to use one-hot matrices so that I could focus on other aspects of the other project. So let‚Äôs talk about them for a bit.&lt;/p&gt;
&lt;h2 class='section-title' id=enter-the-one-hot-matrix&gt;&lt;a href='#enter-the-one-hot-matrix' class='section-inner'&gt;Enter the (one-hot) matrix&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One-hot matrices are called ‚Äúone-hot‚Äù because they each embody one dimension of difference from each other; each matrix has one distinguishing (‚Äúhot‚Äù) characteristic. We can take all the data in our system and represent them using this flattened system.
As an example, let‚Äôs take a couple of lines from &lt;a href="https://www.python.org/dev/peps/pep-0020/"&gt;PEP 20&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="code plaintext"&gt;&lt;code&gt;Complex is better than complicated.
Flat is better than nested.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we represent that in a one-hot matrix?&lt;/p&gt;

&lt;p&gt;We begin by tokenizing the utterance; that is, breaking it into words. We end up with&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'complex'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'complicated'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'flat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'nested'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a lookup dictionary of all  the unique words. We now have:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s"&gt;'complex'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'complicated'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'flat'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'nested'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Count doesn‚Äôt matter; order doesn‚Äôt matter. Each token just needs a unique identifier.
Now to create the matrices: each token needs to be transformed from a string into an array. Each array is of the length of the dictionary, and each value in the dictionary that‚Äôs &lt;em&gt;not&lt;/em&gt; the value of the current token is represented by a 0. The value of the token is represented with a 1. ‚ÄúComplex is better than complicated‚Äù would look like:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#complex&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#is&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#better&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#than&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#complicated&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can deal with the tokens in a uniform way, since they‚Äôre all represented by isomorphic data structures, and once we‚Äôre done we can look up their values using the dictionary we made earlier.&lt;/p&gt;

&lt;p&gt;One-hot matrices get large quickly. In my example, I‚Äôm ‚Äúonly‚Äù using the top 3000 most-commonly occurring words in the training corpus. This means that each word becomes represented by an array 3000 items long üò¨ Whether using one-hot matrices or not, we‚Äôre reckoning with a ton of data, but one-hot matrices are ideally used for a small or finite dataset. In MNIST, for example, a one-hot matrix is used to encode information about whether an image represents a digit from 0 to 9. All the arrays are kept nice and small to a length of 10.&lt;/p&gt;
&lt;h2 class='section-title' id=lets-get-cooking&gt;&lt;a href='#lets-get-cooking' class='section-inner'&gt;Let‚Äôs get cooking&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Enough preamble; time to get started actually building our neural net! Our work will be based on &lt;a href="https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py"&gt;the Reuters example&lt;/a&gt; in the Keras github repo, but we‚Äôll use our own data set and make a couple more tweaks on the way.&lt;/p&gt;

&lt;p&gt;I will be going over all the code in detail, but &lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;I have published it in full in a gist&lt;/a&gt;. It doesn&amp;#39;t have a ton of backstory but it does have all the code in one nice place for you.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Neural nets can take anywhere from a few moments to days to train, depending on your hardware and on how large and/or complex your dataset is. This net took me ~60 minutes to train on a mid-2015 MacBook Pro (and it got NOISY üòÖ ). My point is, you probably won‚Äôt want to have to train the net every single time you want to use it. The last step of our training script will also save the net so that we can ‚Äúboot it up‚Äù quickly from another script when we actually want to consult it.&lt;/p&gt;

&lt;p&gt;Let‚Äôs worry about the training script first. We‚Äôll need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get our data into a usable format&lt;/li&gt;
&lt;li&gt;Build our neural net&lt;/li&gt;
&lt;li&gt;Train it with said data&lt;/li&gt;
&lt;li&gt;Save the neural net for future use&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class='section-title' id=organizing-our-data&gt;&lt;a href='#organizing-our-data' class='section-inner'&gt;Organizing our data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We‚Äôre using the &lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;Twitter Sentiment Analysis Dataset&lt;/a&gt; available via ThinkNook. Be prepared; this dataset is &lt;em&gt;extremely large&lt;/em&gt; and may take forever-ish to open in Excel (I had more success with Numbers).&lt;/p&gt;

&lt;p&gt;One you open it, you can see a massive table that starts with this:&lt;/p&gt;
&lt;div class='table-wrap'&gt;&lt;table&gt;&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;ItemID&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sentiment&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;SentimentSource&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;SentimentText&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;is so sad for my APL friend.............&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;I missed the New Moon trailer...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;omg its already 7:30 :O&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;The only columns we‚Äôre interested in here are 1 and 3 ‚Äî we‚Äôll be training our net on inputs of column &lt;code&gt;SentimentText&lt;/code&gt; with outputs of &lt;code&gt;Sentiment&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We need to convert &lt;code&gt;SentimentText&lt;/code&gt; utterances to one-hot matrices, and create a dictionary of all the words we keep track of. Here, the &lt;code&gt;numpy&lt;/code&gt; library is your friend. I hadn‚Äôt used it much before this but it‚Äôs super powerful and has some really useful built-in utilities.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;

&lt;span class="c"&gt;# extract data from a csv&lt;/span&gt;
&lt;span class="c"&gt;# notice the cool options to skip lines at the beginning&lt;/span&gt;
&lt;span class="c"&gt;# and to only take data from certain columns&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'path/to/your/data.csv'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;','&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;usecols&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# create our training data from the tweets&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;# index all the sentiment labels&lt;/span&gt;
&lt;span class="n"&gt;train_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, we‚Äôve indexed all of our data; time to use Keras to make it machine-friendly.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;

&lt;span class="c"&gt;# only work with the 3000 most popular words found in our dataset&lt;/span&gt;
&lt;span class="n"&gt;max_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3000&lt;/span&gt;

&lt;span class="c"&gt;# create a new Tokenizer&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# feed our tweets to the Tokenizer&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Tokenizers come with a convenient list of words and IDs&lt;/span&gt;
&lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;
&lt;span class="c"&gt;# Let's save this out so we can use it later&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'dictionary.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# one really important thing that `text_to_word_sequence` does&lt;/span&gt;
    &lt;span class="c"&gt;# is make all texts the same length -- in this case, the length&lt;/span&gt;
    &lt;span class="c"&gt;# of the longest text in the set.&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_to_word_sequence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;allWordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="c"&gt;# for each tweet, change each token to its ID in the Tokenizer's word_index&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;wordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordIndices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# now we have a list of all tweets converted to index arrays.&lt;/span&gt;
&lt;span class="c"&gt;# cast as an array for future usage.&lt;/span&gt;
&lt;span class="n"&gt;allWordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# create one-hot matrices out of the indexed tweets&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sequences_to_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'binary'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# treat the labels as categories&lt;/span&gt;
&lt;span class="n"&gt;train_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;‚ú® Cooooooooooooool. ‚ú® Now you have training data and labels that you‚Äôll be able to pipe right into your neural net.&lt;/p&gt;
&lt;h2 class='section-title' id=making-a-model&gt;&lt;a href='#making-a-model' class='section-inner'&gt;Making a model&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Keras makes building neural nets as simple as possible, to the point where you can add a layer to the network in short line of code. Here‚Äôs how I built my net:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Activation&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_words&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks simple enough. What does it mean?? üåàüåà&lt;/p&gt;

&lt;p&gt;Keras‚Äô &lt;code&gt;Sequential()&lt;/code&gt; is a simple type of neural net that consists of a ‚Äústack‚Äù of layers executed in order.&lt;/p&gt;

&lt;p&gt;If we wanted to, we could make a stack of only two layers (input and output) to make a complete neural net ‚Äî without hidden layers, it wouldn‚Äôt be considered a deep neural net.&lt;/p&gt;

&lt;p&gt;The input and output layers are the most important, since they determine the overall shape of the neural net. You need to know what kind of input to expect, and what kind of output you want.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/nnet2.png" alt="A diagram of a neural net used to identify digits using MNIST data." title="A page from the Perspectiva Corporum Regularium" width="537px" height="447px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A representation of a neural net for identifying digits using the MNIST dataset. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Out network will mostly consist of &lt;code&gt;Dense&lt;/code&gt; layers ‚Äî the ‚Äústandard‚Äù, linear neural net layer of inputs, weights, and outputs.&lt;/p&gt;

&lt;p&gt;In our case, we‚Äôre inputting a sentence that will be converted to a one-hot matrix of length &lt;code&gt;max_words&lt;/code&gt; ‚Äî here, 3000. We also include how many outputs we want to come out of that layer (512, for funsies) and what kind of maximization (or ‚Äúactivation‚Äù) function to use.&lt;/p&gt;

&lt;p&gt;Activation functions are used when training the network; they tell the network how to judge when a weight for a particular node has created a good fit. In the first layer, I use &lt;code&gt;relu&lt;/code&gt; (also for funsies). Activation functions differ, mostly in speed, but all the ones available in Keras and TensorFlow are viable; feel free to play around with them. If you don‚Äôt explicitly add an activation function, that layer will use a linear one.&lt;/p&gt;

&lt;p&gt;Our output layer consists of two possible outputs, since that‚Äôs how many categories our data could get sorted into. If you use a neural net to predict rather than classify, you‚Äôre actually creating a neural net with one possible output ‚Äî the prediction.&lt;/p&gt;

&lt;p&gt;In between the input and output layers, we have one more &lt;code&gt;Dense&lt;/code&gt; layer and two &lt;code&gt;Dropout&lt;/code&gt; layers. Dropouts are used to randomly remove data, which can help avoid overfitting. Overfitting can happen when you keep training on the same or overly-similar data ‚Äî as you train, your accuracy will hold steady or drop instead of rising.&lt;/p&gt;

&lt;p&gt;As the last step before training, we need to compile the network:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'adam'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'accuracy'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifying that we want to collect the &lt;code&gt;accuracy&lt;/code&gt; metric will give us really helpful live output as we train our model.&lt;/p&gt;
&lt;h2 class='section-title' id=how-to-train-your-network&gt;&lt;a href='#how-to-train-your-network' class='section-inner'&gt;How to train your network&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is some tiny code that will take a while to run:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;validation_split&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We‚Äôre fitting (training) our model off of inputs &lt;code&gt;train_x&lt;/code&gt; and categories &lt;code&gt;train_y&lt;/code&gt;. We evaluate data in groups of &lt;code&gt;batch_size&lt;/code&gt;, checking the network‚Äôs accuracy, tweaking node weights, and then running through another batch. Small batches let you train networks much more quickly than if you tried to use a batch the size of your entire training dataset.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;epochs&lt;/code&gt; is how many times you do this batch-by-batch splitting. I‚Äôve found 5 to be good in this case; I tried 7, but ended up overfitting.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;validation_split&lt;/code&gt; says how much of your input you want to be reserved for testing data ‚Äî essential for seeing how accurate your network is at that point. Recommended training-to-test ratios are 80:20 or 90:10. You don‚Äôt want to compromise the size of your training corpus, but you need enough test data to actually see how your net is doing.&lt;/p&gt;

&lt;p&gt;Now go get coffee, or maybe a meal. And if you‚Äôre on a laptop, make sure it‚Äôs plugged in ‚Äî training can be a real battery killer ‚ò†Ô∏è&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/perspectiva.jpg" alt="An etching of a fractured geometric figure propped up on a platform surrounded by quare pyramids and a crucifix." title="A page from the Perspectiva Corporum Regularium" width="1191px" height="800px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;If you need something to lose yourself in for about an hour, I suggest the &lt;em&gt;&lt;a href="http://digital.slub-dresden.de/werkansicht/dlf/12830/"&gt;Perspectiva Corporum Regularium&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;As you train the neural net, Keras will output running stats on what epoch you‚Äôre in, how much time is left in that epoch of training, and current accuracy. The value to watch is not &lt;code&gt;acc&lt;/code&gt; but &lt;code&gt;val_acc&lt;/code&gt;, or Validation Accuracy. This is your neural net&amp;#39;s score when predicting values for data in your validation split.&lt;/p&gt;

&lt;p&gt;Your accuracy should start out low per epoch and rise throughout the epoch; it should increase at least a little across epochs. If your accuracy starts decreasing, you‚Äôre overfitting.&lt;/p&gt;

&lt;p&gt;This is some sample output from training using this code over five epochs:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;780&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4947&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7610&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4500&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7884&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;850&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4737&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7760&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4481&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7902&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;788&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4662&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7817&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4446&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7921&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;819&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4607&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7859&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4471&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7921&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;829&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4569&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7887&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4439&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7927&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our accuracy increases from 78.8% accurate by the end of Epoch 1 to 79.3% at the end of Epoch 5.&lt;/p&gt;

&lt;p&gt;&lt;div class="image-wrap image-primary"&gt;
    &lt;img src="/images/keras/noice.gif" alt="NOICE!" title="NOICE!!" width="500px" height="201px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;h2 class='section-title' id=saving-your-model&gt;&lt;a href='#saving-your-model' class='section-inner'&gt;Saving your model&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Once you‚Äôre done training, it‚Äôs time to save your net so that you don‚Äôt have to keep repeating all of those steps.&lt;/p&gt;

&lt;p&gt;Your model gets saved in two parts: One is the model‚Äôs structure itself; the other is the weights used in those model‚Äôs nodes.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.h5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that‚Äôs it!&lt;/p&gt;
&lt;h2 class='section-title' id=party-time&gt;&lt;a href='#party-time' class='section-inner'&gt;Party time&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Finally you can use your neural net! In a new file, you‚Äôll open the model, its weights, and your dictionary, and then put those together to classify text. We‚Äôre going to go over the whole file at once because I‚Äôm excited to actually use it:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;model_from_json&lt;/span&gt;

&lt;span class="c"&gt;# we're still going to use a Tokenizer here, but we don't need to fit it&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# for human-friendly printing&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'negative'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'positive'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c"&gt;# read in our saved dictionary&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'dictionary.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# this utility makes sure that all the words in your input&lt;/span&gt;
&lt;span class="c"&gt;# are registered in the dictionary&lt;/span&gt;
&lt;span class="c"&gt;# before trying to turn them into a matrix.&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_to_word_sequence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;wordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;wordIndices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"'&lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;s' not in training corpus; ignoring."&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wordIndices&lt;/span&gt;

&lt;span class="c"&gt;# read in your saved model structure&lt;/span&gt;
&lt;span class="n"&gt;json_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loaded_model_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;# and create a model from that&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loaded_model_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# and weight your nodes with your saved values&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.h5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# okay here's the interactive part&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;evalSentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;raw_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Input a sentence to be evaluated, or Enter to quit: '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evalSentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;

    &lt;span class="c"&gt;# format your input for the neural net&lt;/span&gt;
    &lt;span class="n"&gt;testArr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evalSentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sequences_to_matrix&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;testArr&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'binary'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;# predict which bucket your input belongs in&lt;/span&gt;
    &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;# and print it for the humons&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;s sentiment; &lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;f&lt;/span&gt;&lt;span class="si"&gt;%% &lt;/span&gt;&lt;span class="s"&gt;confidence"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this file, you create a new model using the saved structure, and then you get a little command prompt for input to classify.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.predict()&lt;/code&gt; takes what you give it, runs it through the trained neural net, and gives you a reading of how confident it is that that input belongs in each output bucket. In our case, we have two outputs, so we have two confidence estimations that range from 0 to 1; whichever one is higher is the network‚Äôs ultimate classification of that data.&lt;/p&gt;
&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: It's alive! :D
positive sentiment; 80.760884% confidence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;üëåüèª&lt;/p&gt;
&lt;h2 class='section-title' id=you-did-it&gt;&lt;a href='#you-did-it' class='section-inner'&gt;You did it!&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You‚Äôve trained your first neural net that evaluates text as expressing positive or negative sentiment. And it works well(ish):&lt;/p&gt;
&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: That went better than expected
positive sentiment; 56.355631% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: That did not go as expected
negative sentiment; 86.936867% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;h2 class='section-title' id=next-steps&gt;&lt;a href='#next-steps' class='section-inner'&gt;Next steps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;div class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/party.gif" alt="" title="Thanks for sticking with me through all of that." width="480px" height="360px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This is a really basic neural net, and there‚Äôs a lot more I‚Äôd like to investigate. Among other things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can deploy machine learning models to the cloud using &lt;a href="https://cloud.google.com/ml-engine/"&gt;Google Cloud ML&lt;/a&gt; ‚Äî what could I do if I ported my local machine to operate on Google Cloud Platform, or if I started building new models using that service?&lt;/li&gt;
&lt;li&gt;I chose the option of less-powerful word indexing by using one-hot matrices instead of vector embeddings. What accuracy could I get if I started using the latter?&lt;/li&gt;
&lt;li&gt;Classification is great but I‚Äôd really love to work on text &lt;em&gt;generation&lt;/em&gt;, guessing at the next most-likely word in the sequence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of the code in this post, plus the requirements file, is up &lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;in a Github gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Happy coding! And thanks for following me on this journey üåü&lt;/p&gt;
&lt;h2 class='section-title' id=coda-how-well-does-it-work-and-more-on-data&gt;&lt;a href='#coda-how-well-does-it-work-and-more-on-data' class='section-inner'&gt;Coda: How well does it work? And more on data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I‚Äôm super proud of this neural net but 79% accuracy means that it‚Äôs wrong 21% of the time.&lt;/p&gt;
&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: I wish I could show you when you are lonely or in darkness the astonishing light of your own being
negative sentiment; 87.021768% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: foo bar
positive sentiment; 62.751633% confidence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ü§î ü§î Those don‚Äôt look quite right.&lt;/p&gt;

&lt;p&gt;We can always build better models, but a lot of this is &lt;a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out"&gt;Garbage In, Garbage Out&lt;/a&gt;. The dataset I used is incredibly large, but looking through it, there are classifications I don‚Äôt agree with, such as marking &lt;code&gt;... health class (what a joke!)&lt;/code&gt; as positive. Without bringing compensation algorithms into the mix, your neural net can only be as accurate as your training data.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;On that note, I encourage you to examine your training data closely. What biases or prejudices might have influenced that information? Those biases will also be present, explicitly or implicitly, in your output.&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;The past is a very racist place. And we only have data from the past to train Artificial Intelligence.
        &lt;cite&gt;
          Trevor Paglen&lt;/p&gt;

        &lt;/cite&gt;
      &lt;/blockquote&gt;
    
&lt;p&gt;This doesn‚Äôt mean that any and all data are inherently biased and therefore unusable ‚Äî we just need to be aware of that bias and work to eradicate it. Researchers from Boston University and Microsoft have &lt;a href="https://arxiv.org/abs/1607.06520"&gt;created ways to work with appropriately gendered analogies without reinforcing gender stereotypes&lt;/a&gt;. &lt;a href="https://artificialintelligencenow.com/"&gt;AI Now&lt;/a&gt; is an entire research organization dedicated to teasing out the biases and impacts of artificial intelligence and machine learning.&lt;/p&gt;

&lt;p&gt;ML is powerful and we can make amazing things with it. Let‚Äôs use it to create a more equitable future.&lt;/p&gt;
&lt;h2 class='section-title' id=downloads&gt;&lt;a href='#downloads' class='section-inner'&gt;Downloads&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>The Art of Gitting Gud</title>
    <link rel="alternate" href="https://vgpena.github.io/the-art-of-gitting-gud/"/>
    <id>https://vgpena.github.io/the-art-of-gitting-gud/</id>
    <published>2017-08-13T15:23:00-07:00</published>
    <updated>2017-08-13T16:06:08-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">I have always liked to write, so I thought that blogging would come naturally to me. Time and focus, though, have proven far more important than a way with words. It's time to face this reality and git gud.</summary>
    <content type="html">&lt;p&gt;It&amp;#39;s been a while since I posted something here. This has not been for lack of exciting things in my life ‚Äî I wrote Arduino for the first time, trained my first neural net, and polished off a six-month-long React project.&lt;/p&gt;

&lt;p&gt;So why, then, no reports on these interesting and possibly trendy adventures? More difficult than debugging &lt;code&gt;char *&lt;/code&gt; Arrays, more daunting than teaching a machine to classify speech, is sitting down and actually writing about all of it.&lt;/p&gt;

&lt;p&gt;One main reason that I created this blog was to get better at writing. I love to write, and I&amp;#39;ve practiced it in one form or another since elementary school. Since writing is filed under &amp;quot;things I enjoy&amp;quot;, I figured that once I made a blog, posts would flow to fill its shape.&lt;/p&gt;

&lt;p&gt;I had forgotten, however, how vital it is not just to like a thing but to &lt;em&gt;allow&lt;/em&gt; yourself to like that thing. I assumed that as I did interesting things, prose would generate itself almost as a side effect, fully-formed and ready to publish.&lt;/p&gt;

&lt;p&gt;Reality has not borne this out.&lt;/p&gt;

&lt;p&gt;My writing ability ‚Äî that is, my aptitude for stringing words together; for forming a narrative ‚Äî will always and forever need improving, but it&amp;#39;s acceptable. (I&amp;#39;ve heard that the worst code ever written is the code that you yourself wrote six months ago. I hope that my writing follows this pattern.)&lt;/p&gt;

&lt;p&gt;But there&amp;#39;s another component of writing ability, and this is where I fall flat. This component has nothing to do with mastery of language; it&amp;#39;s concerned with letting myself &lt;em&gt;at&lt;/em&gt; language in the first place. What kind of time, what kind of space do I give myself to write? Do I allow myself to focus for long enough to even jot down an outline?&lt;/p&gt;

&lt;p&gt;It was late in freshman year of college that I realized how effortless it was to be purely a consumer and not a creator. I remember that thought (in an uncommon way) slipping small and crystalized into my mind, a pressure against my skull. I&amp;#39;ve tried to stay aware of that fact, and to keep myself in check. &lt;em&gt;Create; make something; remember&lt;/em&gt; how &lt;em&gt;to make something.&lt;/em&gt; What if I lose practice? What if I forget how to create?&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I began to understand the effort that goes into creation, but I hadn&amp;#39;t yet been tested for my own willingness to expend that effort. Again, I was in college. I no longer had time for art projects, but I &lt;em&gt;was&lt;/em&gt; ordered to spend a month comparing editions of the &lt;em&gt;Quijote&lt;/em&gt;; I was told to write translations and program translators and dissect the differences between turn-of-the-century European authors. I began to learn how to create my own time, my own focus, but I did not achieve proficiency in it. Outside of my mandated, protected time, I dove into the internet, read books aimlessly, and, after a breakup, binge-watched all of &lt;em&gt;Avatar: the Last Airbender&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;When all was said and done, I was very good at writing when someone else made me do it. Did I have my own passions; did I find things interesting? Absolutely. But I wasn&amp;#39;t lifting a finger to express or magnify those pursuits.&lt;/p&gt;

&lt;p&gt;Years later, I began to understand what adults were getting at when they acted impressed that so-and-so was in a community choir, or taking a German class, or sowing a garden. They weren&amp;#39;t necessarily blown away by someone&amp;#39;s vocal prowess, but they deeply understood the power of inertia. It&amp;#39;s infinitely harder to do anything than it is to do nothing.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;When I created this blog, I initially &lt;em&gt;did&lt;/em&gt; allow myself to write. I recorded post ideas as they occurred to me; I drafted outlines during downtime at work; I blocked out time on alternating weekends to do the actual writing. &lt;em&gt;This&lt;/em&gt;, a voice whispered, &lt;em&gt;is temporary. Soon you&amp;#39;ll be so good at writing that it&amp;#39;ll just happen.&lt;/em&gt; I imagined that, with some practice, I&amp;#39;d be able to be struck by an idea, transmute it into words, edit a bit, and send it off into the internet vastness.&lt;/p&gt;

&lt;p&gt;I may be able to do this someday, but not yet. I made the mistake of thinking that I was almost there ‚Äî &lt;em&gt;this week&lt;/em&gt; I could go away for the weekend, make no allowance for writing, and figure something out in an hour or two on Monday evening. Never mind that previous blog posts had required working four hours a day for two, three days in a row. Never mind that Monday evening would find me on the couch watching a show &lt;em&gt;to unwind&lt;/em&gt; because I was &lt;em&gt;so done with work&lt;/em&gt; after getting home from my job and prepping dinner and doing all the miscellaneous things that life requires. I would do it Tuesday? Or the next weekend? Tomorrow Me would definitely feel peppier, more inspired.&lt;/p&gt;

&lt;p&gt;This post took me days to write. Those days were not spent drafting and editing;  those days lived in the space between when this urgent, personal idea occured to me and when I put pen to paper and actually sat down and wrote the damn thing.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;And finally, I circle back around to the title of this post: &lt;em&gt;The art of gitting gud&lt;/em&gt;. I don&amp;#39;t mean Scrooge; it&amp;#39;s not about having a change of heart. I mean getting good like &lt;em&gt;Dark Souls&lt;/em&gt; like a note I have taped to my fridge, GIT GUD.&lt;/p&gt;

&lt;p&gt;&lt;div class="image-wrap image-secondary"&gt;
    &lt;img src="/images/gitgud-tumblr.png" alt="A screenshot of a tumblr update. Text says: 'I'm afraid he's been infected with the casual. He's just going to have to...to--'. Image below text is of a doctor wearing a knight's helmet, pointing to a clipboard that has 'git gud' written on it" title="A common variant of git gud" width="600px" height="408px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&amp;quot;git gud&amp;quot; entered my consciousness when the notoriously difficult videogame &lt;em&gt;Dark Souls&lt;/em&gt; was released; it was advice frequently offered to players seeking help. While on the surface, it&amp;#39;s unhelpful and possibly adversarial, it communicates an important truth: sometimes there are no shortcuts to getting what you want. The only way forward is to practice, to sink in the hours and the effort until, someday, you git gud.&lt;/p&gt;

&lt;p&gt;Writing is something that I will improve only through practice. I need to respect it more, acknowledge its difficulty, and invest the focus and the time into working on it.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know how long it will be for me to feel like I&amp;#39;ve improved; I don&amp;#39;t know how long until I&amp;#39;ve &lt;em&gt;actually&lt;/em&gt; improved. But for me, this is difficult. And the only way to make it easier is to git gud.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>We RISE 2017: yes please</title>
    <link rel="alternate" href="https://vgpena.github.io/we-rise-2017/"/>
    <id>https://vgpena.github.io/we-rise-2017/</id>
    <published>2017-07-01T14:06:00-07:00</published>
    <updated>2017-07-01T16:13:02-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">I gave my first tech talk at We RISE's inaugural conference on June 23-24, 2017. Here are my main takeaways from and thoughts on the conference.</summary>
    <content type="html">&lt;p&gt;Last week, I &lt;a href="https://docs.google.com/presentation/d/1t2-ebDUL6r09tO1s18UXWvALtZg-cQebSZpEmikYUNE/edit?usp=sharing"&gt;spoke&lt;/a&gt; at &lt;a href="https://werise.tech/"&gt;We RISE conf&lt;/a&gt; in Atlanta, GA. I&amp;#39;ll write a post on that speaking experience, but first I&amp;#39;d like to talk about the conference itself.&lt;/p&gt;

&lt;p&gt;We RISE is a Women In Tech conference put together by &lt;a href="https://www.womenwhocode.com/atl"&gt;Women Who Code ATL&lt;/a&gt;. This year was its debut year and it &lt;strong&gt;kicked ass&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been to talks and conferences with pretty strong social undertones, but this was the first time I&amp;#39;d been to a major event overtly aimed at social issues and progress in tech. The sessions were pretty evenly split between more &amp;quot;typical&amp;quot; tech talks (e.g, intro to Rust) and more socially-oriented ones (e.g., creating an allyship program at your company). No matter the speaker or the subject matter, there was always an undercurrent: let&amp;#39;s get more women into and excelling in tech.&lt;/p&gt;

&lt;p&gt;We RISE was excellent, for many reasons, and I&amp;#39;ll try to break them down a bit.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;To start off with something banal and probably obvious, there were a lot of women there. Women must have made up 95 percent of attendees. I didn&amp;#39;t think I would notice, but I did, and it was amazing.&lt;/p&gt;

&lt;p&gt;I won&amp;#39;t dive deeply into this right now, but as a developer who&amp;#39;s a woman, I&amp;#39;m used to being in a minority. I&amp;#39;ve had the good fortune (or maybe instinct) to end up working with respectful, smart, helpful, supportive, amazing people. They just happen to be mostly men.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve never had a boss who wasn&amp;#39;t a man. For five years, I worked directly alongside zero women. That spell ended last month, for which I am grateful.&lt;/p&gt;

&lt;p&gt;With this as my norm, it was subtly mind-blowing to spend two days answering to and speaking with almost exclusively women. My paper was accepted by a woman; I got speaker updates from women; women answered my questions about scheduling. I went to talks given by women and my talk, in turn, was mostly attended by women. I answered womens&amp;#39; questions about WebSockets, read womens&amp;#39; slide decks, and chatted with women about programming, Atlanta, and our careers and ambitions.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;The talks specifically about social issues were valuable and necessary. It was the tech sessions, however, which got me really excited. A talk can focus completely on tech and yet be an instrument of social progress when it is given in the right context.&lt;/p&gt;

&lt;p&gt;This was the case at We RISE. Looking around the room at almost exclusively other women, I felt pride to realize that it would be &lt;em&gt;us&lt;/em&gt; going back to work on Monday energized, full of connections and ideas, ready to share and talk and create change. There would be a wave of women suggesting techniques, sharing links to new tools, and rounding up parners to work on side projects.&lt;/p&gt;

&lt;p&gt;(As a sidenote, I realized that the flipside of this is also true -- every time a conference is unwelcoming, exclusive, etc., women and other marginalized groups are taking two hits: not only are they not going to the conference itself but they also are less likely to be showing up at work with new ideas to present, discuss, and implement.)&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I also met people from all kinds of backgrounds who got into tech for all kinds of reasons. My world mostly consists of under-40s who have worked in tech for many years. They mostly don&amp;#39;t have comp sci degrees but they have finished college, are childless, and have had generally stable lives.&lt;/p&gt;

&lt;p&gt;At We RISE, I met record numbers of people who didn&amp;#39;t fit this profile. There were droves of code school students, people entering tech in their 40s, people with children. Their concerns and goals were totally different from mine and those of most of my colleagues. They chose coding bootcamps that worked with their schedules of full-time work and childcare. They applied for jobs not that promised glory or stock options but healthcare and regular hours.&lt;/p&gt;

&lt;p&gt;Tech from the west coast and NYC tends to be splashy and dramatic. Tech from Atlanta stays out of the headlines and, honestly, it keeps such a low profile that I was surprised that there was a tech conference happening there at all.&lt;/p&gt;

&lt;p&gt;We RISE reminded me that these people are my compatriots. They&amp;#39;re my extended family. And I&amp;#39;m going to work harder to connect with and support all elements of our community.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;strong&gt;Anyway&lt;/strong&gt;: We RISE was the best and it&amp;#39;s doing some really important things for women and tech and women in tech. If you went, well done! If you didn&amp;#39;t, go next year! I had an incredible time and I am so thankful that I had the opportunity to meet rad people, give my first ever talk, and spread some inspiration and knowledge.&lt;/p&gt;

&lt;p&gt;Stay tuned, lovely people, for a post on my talk itself and lessons learned by a first-time speaker üòò&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Winning with CSS Variables</title>
    <link rel="alternate" href="https://vgpena.github.io/winning-with-css-variables/"/>
    <id>https://vgpena.github.io/winning-with-css-variables/</id>
    <published>2017-04-30T14:30:00-07:00</published>
    <updated>2017-05-12T16:09:17-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">CSS variables now enjoy wide cross-browser support. But what are they and why should we use them?</summary>
    <content type="html">&lt;p&gt;CSS variables, like variables in any programming language, let us reference the same values over and over. As of April 2017, they are supported by &lt;a href="http://caniuse.com/#feat=css-variables"&gt;all modern browsers&lt;/a&gt; and are an effective way to write tight, clean styles.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be walking through the basics of CSS variables, how they are different from Sass variables, and how to provide legacy support.&lt;/p&gt;
&lt;h2 class='section-title' id=how-to-use-them&gt;&lt;a href='#how-to-use-them' class='section-inner'&gt;How to use them&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Any CSS property -- color, size, position, etc. -- can be stored in a CSS variable. Their names are all prefixed with &lt;code&gt;--&lt;/code&gt;, and you declare them by adding them to an element right where you add its other styles:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;body&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="py"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="py"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You refer to a CSS variable by wrapping it in &lt;code&gt;var()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="nl"&gt;text-decoration-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at this CSS in your web inspector, you&amp;#39;ll see that these variables are not being aliased or transpiled in any way -- your browser will tell you that an &lt;code&gt;a&lt;/code&gt;&amp;#39;s color is &lt;code&gt;--primary&lt;/code&gt;, not the hex value itself.&lt;/p&gt;

&lt;p&gt;When you use a CSS variable, you can also pass in an optional default value:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;color&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;var&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;--primary&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;#7F583F&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This default value will be used if the CSS variable is not defined or available in the current scope.&lt;/p&gt;
&lt;h2 class='section-title' id=scoping-and-the-cascade&gt;&lt;a href='#scoping-and-the-cascade' class='section-inner'&gt;Scoping and the cascade&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;CSS variables act like a normal style property; a variable is available anywhere down the cascade.&lt;/p&gt;

&lt;p&gt;For example, these variables can be used by anything on the entire page:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;body&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="py"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="py"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And these will only be defined within elements with a certain class:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nc"&gt;.content&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="py"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="py"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this second example, if you try to use &lt;code&gt;--primary&lt;/code&gt; outside of a &lt;code&gt;.content&lt;/code&gt; element, the page will still render but that style will not be applied.&lt;/p&gt;
&lt;h2 class='section-title' id=the-paradigm-custom-css-properties&gt;&lt;a href='#the-paradigm-custom-css-properties' class='section-inner'&gt;The Paradigm: custom CSS properties&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You may have noticed a theme so far, which is that variables act just like other CSS properties. You declare variables where you set properties, they cascade in the same way, and so on.&lt;/p&gt;

&lt;p&gt;This is because CSS variables are actually nothing more than custom properties. The only difference between &lt;code&gt;--primary&lt;/code&gt; and &lt;code&gt;position&lt;/code&gt; is that &lt;code&gt;position&lt;/code&gt; always means something specific and directly affects rendering, whereas &lt;code&gt;--primary&lt;/code&gt; does nothing until it is explicitly used.&lt;/p&gt;

&lt;p&gt;CSS variables being supported by a browser means that the browser allows the user to set arbitrary, namespaced CSS properties. &lt;strong&gt;This is really exciting.&lt;/strong&gt; Just like how it&amp;#39;s exciting that media queries let us get away from resize listeners in JavaScript, CSS variables are opening the door to a future that relies less on JS and preprocessors. Speaking of which...&lt;/p&gt;
&lt;h2 class='section-title' id=better-than-sass-theming&gt;&lt;a href='#better-than-sass-theming' class='section-inner'&gt;Better than Sass: theming&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;CSS variables aren&amp;#39;t analogous to Sass variables, and in some ways, the former are vastly preferable to the latter. One of these situations is when you&amp;#39;re looking to swap out themes. On &lt;a href="http://violet.is"&gt;my personal site&lt;/a&gt;, I randomly theme the page each time it is loaded; the user can end up with any of nine color schemes.&lt;/p&gt;

&lt;p&gt;This is easily done using Sass. &lt;a href="http://now.violet.is/color-scheming"&gt;Store your color combinations in Sass maps&lt;/a&gt;, loop over them, and you can quickly create a bunch of classes that you can apply to the page:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nc"&gt;.theme-1&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="err"&gt;a&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;text-decoration-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="nc"&gt;.theme-2&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="err"&gt;a&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#D51522&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;text-decoration-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F4F6D8&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="c"&gt;/* etc */&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The CSS is easy to generate, but in the case of nine variations, you end up with eight classes&amp;#39; worth of styling that remains unused.&lt;/p&gt;

&lt;p&gt;CSS variables can achieve the same effect with no &amp;quot;extra&amp;quot; CSS. In this case, instead of using JavaScript to add a class to the page, you can use it to set specific CSS variables:&lt;/p&gt;
&lt;pre class="code javascript"&gt;&lt;code&gt;
&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;style&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setProperty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'--primary'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'#7F583F'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;style&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setProperty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'--secondary'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'#F7EFD2'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These changes are picked up by every element in the cascade that uses that variable. Your styles stay cleaner and you don&amp;#39;t have to go through the middleman of applying classes.&lt;/p&gt;
&lt;h2 class='section-title' id=better-than-sass-media-queries&gt;&lt;a href='#better-than-sass-media-queries' class='section-inner'&gt;Better than Sass: media queries&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In Sass, redefining variables within media queries is something that you Just Can&amp;#39;t Do. For instance, maybe you want to swap link colors when you get to a breakpoint. You may be tempted to redeclare the variables themselves inside of the media query:&lt;/p&gt;
&lt;pre class="code scss"&gt;&lt;code&gt;
&lt;span class="nv"&gt;$primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mh"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mh"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nv"&gt;$primary&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="nl"&gt;text-decoration-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nv"&gt;$secondary&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="k"&gt;@media&lt;/span&gt; &lt;span class="n"&gt;screen&lt;/span&gt; &lt;span class="nf"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min-width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;768px&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nv"&gt;$primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mh"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nv"&gt;$secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mh"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This, sadly, doesn&amp;#39;t work in Sass, since Sass is a preprocessor and can&amp;#39;t know anything about the conditions under which its output is used.&lt;/p&gt;

&lt;p&gt;This pattern &lt;em&gt;can&lt;/em&gt; be used with CSS variables, though:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;body&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="py"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="py"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="nl"&gt;text-decoration-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;@media&lt;/span&gt; &lt;span class="n"&gt;screen&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min-width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;768px&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;body&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="py"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="m"&gt;#F7EFD2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="py"&gt;--secondary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works with CSS variables because all change is happening in-browser, and the variables &lt;em&gt;do&lt;/em&gt; know about the conditions under which they are being used.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;This said, I ‚ù§Ô∏è Sass and a combination of these tools is way more powerful than each is individually. In fact, I have a great lil mixin further down the page that leverages Sass for declaring CSS variable fallbacks.&lt;/p&gt;
&lt;h2 class='section-title' id=browser-support&gt;&lt;a href='#browser-support' class='section-inner'&gt;Browser support&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;CSS variables have been in Firefox since 2014, in Chrome + Safari since March 2016, and just landed in Edge April 2017! üéâ (Source:  &lt;a href="http://caniuse.com/#feat=css-variables"&gt;CanIUse&lt;/a&gt;.) So the good news is that they&amp;#39;re quite safe; the bad news is that you &lt;em&gt;will&lt;/em&gt; need fallbacks for Edge 14- and, naturally, all of IE.&lt;/p&gt;
&lt;h2 class='section-title' id=providing-fallbacks&gt;&lt;a href='#providing-fallbacks' class='section-inner'&gt;Providing fallbacks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Luckily, the way to provide these fallback styles is the way we have been doing it since time immemorial:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Declare your fallback first and your desired value second, and browsers that support your preferred property will use it. Browsers that don&amp;#39;t, such as IE 11, will still render something acceptable using your fallback value.&lt;/p&gt;
&lt;h2 class='section-title' id=easier-fallbacks-with-sass&gt;&lt;a href='#easier-fallbacks-with-sass' class='section-inner'&gt;Easier fallbacks with Sass&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you&amp;#39;re using Sass, you can automate fallbacks through a Sass mixin. Create a map of your CSS variable names and their values, and then you can look up those values in a mixin that outputs the fallback style and the preferred one.&lt;/p&gt;
&lt;pre class="code scss"&gt;&lt;code&gt;
&lt;span class="nv"&gt;$vars&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;primary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mh"&gt;#7F583F&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="nt"&gt;body&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="na"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nf"&gt;map-get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$vars&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;primary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;@mixin&lt;/span&gt; &lt;span class="nf"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$property&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;$varName&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="nv"&gt;$property&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="nd"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;map-get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nt"&gt;vars&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nt"&gt;varName&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="nv"&gt;$property&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="nd"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;var&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;--&lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="nv"&gt;$varName&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;map-get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nt"&gt;vars&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nt"&gt;varName&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above mixin is used like so:&lt;/p&gt;
&lt;pre class="code scss"&gt;&lt;code&gt;
&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;@include&lt;/span&gt; &lt;span class="nd"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;primary&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and outputs the following CSS:&lt;/p&gt;
&lt;pre class="code css"&gt;&lt;code&gt;
&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;--primary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;#7F583F&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way, if you change &lt;code&gt;--primary&lt;/code&gt; or its fallback, you only need to edit the &lt;code&gt;$vars&lt;/code&gt; map and your styles everywhere will update.&lt;/p&gt;

&lt;p&gt;Please note that you still need to declare your CSS variables somewhere. If you want all of your variables to be available everywhere, you can use more cool Sass to automatically add all the ones in your map to &lt;code&gt;body&lt;/code&gt; or &lt;code&gt;html&lt;/code&gt; ‚ú®&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;If you want to see CSS variables in action, head on over to &lt;a href="http://violet.is/"&gt;my personal site&lt;/a&gt;. I had an amazing time on this small project and I&amp;#39;m looking forward to using them on something much larger ü•Ç&lt;/p&gt;

&lt;p&gt;If you still haven&amp;#39;t had enough of CSS variables, check out &lt;a href="https://developers.google.com/web/updates/2016/02/css-variables-why-should-you-care"&gt;this Google Developers blog post&lt;/a&gt;. They do a great job of sticking to the &amp;quot;CSS variables are custom properties&amp;quot; paradigm.&lt;/p&gt;

&lt;p&gt;Now go forth and style!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Fixing Jest Memory Usage on CircleCI</title>
    <link rel="alternate" href="https://vgpena.github.io/jest-circleci/"/>
    <id>https://vgpena.github.io/jest-circleci/</id>
    <published>2017-04-18T20:14:00-07:00</published>
    <updated>2017-04-18T20:15:48-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">Our CircleCI builds stopped working; the fix was to limit the number of workers Jest could use.</summary>
    <content type="html">&lt;p&gt;If you&amp;#39;re running out of memory or your tests are hanging when you run Jest tests on CircleCI, try running &lt;code&gt;jest --maxWorkers=2&lt;/code&gt; instead of just &lt;code&gt;jest&lt;/code&gt;.&lt;/p&gt;
&lt;h2 class='section-title' id=why&gt;&lt;a href='#why' class='section-inner'&gt;Why?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;On my current project, we&amp;#39;re using &lt;a href="https://facebook.github.io/jest/"&gt;Jest&lt;/a&gt; and &lt;a href="https://github.com/airbnb/enzyme"&gt;Enzyme&lt;/a&gt; to create unit tests which we run as a step in our &lt;a href="https://circleci.com/"&gt;CircleCI&lt;/a&gt; build process. Every time a PR is opened on &lt;a href="https://github.com"&gt;GitHub&lt;/a&gt;, CircleCI rebuilds the project, runs the tests, and lets us know if that PR&amp;#39;s code is stable.&lt;/p&gt;

&lt;p&gt;About 400 tests in, however, an issue emerged -- all tests would pass locally, but the CircleCI build would fail. The CircleCI logs made it clear, though, that no tests were explicitly failing. Something was making the build hang at the test step, it would time out, and the box would consider that build failed.&lt;/p&gt;

&lt;p&gt;We realized that the issue might lie with how Jest spawns child processes. Although test output appears linear, the tests by default run in parallel, using up to as many workers as there are cores on the machine. Maybe Jest was spinning off the wrong number of workers and this was causing memory problems on our CI box.&lt;/p&gt;

&lt;p&gt;The first thing I tried was running &lt;code&gt;jest --runInBand&lt;/code&gt;, which makes Jest run all tests in the same process (&lt;a href="https://facebook.github.io/jest/docs/cli.html"&gt;more info on Jest CLI options&lt;/a&gt;). This turned out not to play well with things that need a headless DOM rendered via &lt;a href="https://github.com/tmpvar/jsdom"&gt;JSDOM&lt;/a&gt;. We use &lt;a href="https://github.com/Khan/aphrodite/"&gt;Aphrodite&lt;/a&gt; for styling, and &lt;code&gt;runInBand&lt;/code&gt; seemed to prevent this DOM from being rendered, which meant Aphrodite couldn&amp;#39;t find any elements to attach itself to.&lt;/p&gt;

&lt;p&gt;Next, I tried &lt;code&gt;jest --maxWorkers=1&lt;/code&gt;, but this seems to do the same thing as &lt;code&gt;runInBand&lt;/code&gt;, and I got the same result. &lt;code&gt;--maxWorkers=2&lt;/code&gt;, however, let the JSDOM render &lt;em&gt;and&lt;/em&gt; all the tests run. I put this change in our &lt;a href="https://circleci.com/docs/1.0/configuration/"&gt;Circle config file&lt;/a&gt; and the subsequent PR was the first one that built properly in over a week.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Since then, our CircleCI builds have been stable. Tests take about 46 seconds to run on our CI box now, compared to 9 seconds locally, where we don&amp;#39;t need to limit workers.&lt;/p&gt;

&lt;p&gt;Thanks for reading this far! Now go out there, write tests, and integrate without fear.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Turning Subreddits Into Chatbots</title>
    <link rel="alternate" href="https://vgpena.github.io/chatbots/"/>
    <id>https://vgpena.github.io/chatbots/</id>
    <published>2017-04-02T22:26:00-07:00</published>
    <updated>2017-04-03T07:36:39-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">We were given two days and the theme of 'echo chamber'. We used subreddits to create chatbots and had them converse.</summary>
    <content type="html">&lt;p&gt;This past weekend, I attended &lt;a href="http://www.arthackday.net/events/echo-chamber"&gt;Art Hack Day: Echo Chamber&lt;/a&gt;. On Friday evening, the participants introduced ourselves and shared ideas; by Sunday evening, I had, as part of a group, created and exhibited Reddit-powered chatbots. It was an amazing, exhilarating weekend and I came out of it tired but more inspired than I thought possible.&lt;/p&gt;
&lt;h2 class='section-title' id=art-hack-the-planet&gt;&lt;a href='#art-hack-the-planet' class='section-inner'&gt;(Art) Hack the Planet&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/ahd/pnca.jpg" alt="A photo of a blocky, light grey building. The letters &amp;quot;PNCA&amp;quot; run up its side." title="The PNCA building in downtown Portland" width="500px" height="667px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;PNCA -- my home for the weekend. &lt;a href="http://gabesimagination.com/"&gt;credit&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.arthackday.net/"&gt;Art Hack Day&lt;/a&gt; (AHD) is a series of events that brings together tecchies and artists. Participants divide into ad hoc groups, work on art projects for a day and a half, and then the weekend culminates in an art show. This time around, we were in the uber-beautiful &lt;a href="http://pnca.edu/"&gt;PNCA&lt;/a&gt; and sponsored in part by their interdisciplinary &lt;a href="http://pnca.edu/makethinkcode"&gt;Make+Think+Code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Part of what&amp;#39;s lovely about AHD is that, as the name implies, they go out of their way to create a balance between art and tech. This isn&amp;#39;t a hack day to make an app or redesign a website; it&amp;#39;s a time to collaborate with people of different skill sets, push your boundaries, and create something interesting but not necessarily useful.&lt;/p&gt;
&lt;h2 class='section-title' id=do-you-want-to-build-a-chatbot&gt;&lt;a href='#do-you-want-to-build-a-chatbot' class='section-inner'&gt;Do You Want to Build a Chatbot?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The theme of this AHD was &amp;quot;Echo Chamber&amp;quot;, particularly with an eye towards the internet and social media, which easily become echo chambers that do little more than affirm our already-held beliefs. This creative prompt got me thinking about chatbots, which I&amp;#39;ve been into since I was introduced to &lt;a href="https://en.wikipedia.org/wiki/SmarterChild"&gt;SmarterChild&lt;/a&gt; in middle school.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/ahd/smarterchild.jpg" alt="A screengrab of an old AIM conversation with SmarterChild." title="A screengrab of a typically awful conversation with SmarterChild" width="242px" height="386px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A typical interaction with SmarterChild. &lt;a href="http://yaleherald.com/bullblog/aim-bots-where-are-they-now/"&gt;source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;On Friday evening, as we went around the room and introduced ourselves, a couple others indicated an interest in chatbots -- &lt;a href="https://twitter.com/ginkko"&gt;Alec Arme&lt;/a&gt; and &lt;a href="http://www.poinyent.com/"&gt;Steve August&lt;/a&gt;. We got together and started brainstorming ideas, which ranged from assigning points to bots&amp;#39; arguments and picking a winner, to creating bots that represented different Greek gods. When we left for the night, we knew that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;we wanted to make several bots, not just one;&lt;/li&gt;
&lt;li&gt;those bots would be trained or given personalities based on different data sources; and&lt;/li&gt;
&lt;li&gt;we wanted those bots to somehow talk to each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point, I should mention that none of us had experience with chatbots, and that none of us had worked together before. We have very different skill sets -- Alec works mostly with VR, Steve specializes in psychology and using tech to create calm, and I do web development and browser-based installations. Whatever our end product would be, we knew it wouldn&amp;#39;t be typical.&lt;/p&gt;
&lt;h2 class='section-title' id=enter-bigquery&gt;&lt;a href='#enter-bigquery' class='section-inner'&gt;Enter BigQuery&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Any bot needs source material, and we had decided that our bots would be trained on internet communities that could be construed as echo chambers. Even before we were sure what the exact training material would be, we knew that we would have to process a &lt;strong&gt;lot&lt;/strong&gt; of data in order to accurately reflect those communities.&lt;/p&gt;

&lt;p&gt;When I think &amp;quot;big data&amp;quot;, I think of &lt;a href="https://cloud.google.com/bigquery/"&gt;Google BigQuery&lt;/a&gt;. I hadn&amp;#39;t worked with it before, but I &lt;strong&gt;had&lt;/strong&gt; had the pleasure of working alongside a team creating &lt;a href="http://www.instrument.com/work/google-next-cloud-platform"&gt;a BigQuery-powered trivia game&lt;/a&gt;, so I was already familiar with BigQuery as a service and knew the basics of how to use it.&lt;/p&gt;

&lt;p&gt;Essentially, BigQuery is a pay-as-you-go service that lets you run SQL queries over humongous data sets in disgustingly little time. It opens the door to people like me, who want to crunch dozens of gigabytes of data, but don&amp;#39;t have access to powerful machines.&lt;/p&gt;

&lt;p&gt;As if this weren&amp;#39;t enough, BigQuery hosts &lt;a href="https://www.reddit.com/r/bigquery/wiki/datasets"&gt;tons of publically available datasets&lt;/a&gt;, so we didn&amp;#39;t have to manually write scrapes of comment sections or format those results to be BigQuery-compatible. (My favorite dataset is this 452GB one of &lt;a href="https://github.com/odota/core/issues/924"&gt;DotA gameplay data&lt;/a&gt;).&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;The dataset that spoke to us the most was the one of &lt;a href="https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit_comments"&gt;all Reddit comments since 2005&lt;/a&gt;. &lt;a href="https://www.reddit.com/"&gt;Reddit&lt;/a&gt;, &amp;quot;the front page of the internet&amp;quot;, is divided into subreddits dedicated to specific hobbies or interests. As such, they can easily become echo chambers where all participants agree with each other and validate each others&amp;#39; beliefs.&lt;/p&gt;

&lt;p&gt;We decided to base our bots off of specific subreddits&amp;#39; comments from December 2016 to February 2017 -- a cool 52.9GB of data in all. But how would we turn this data source into usable material for chatbots?&lt;/p&gt;
&lt;h2 class='section-title' id=creating-a-training-corpus&gt;&lt;a href='#creating-a-training-corpus' class='section-inner'&gt;Creating a Training Corpus&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When I want to generate language, I immediately think of &lt;a href="https://en.wikipedia.org/wiki/Markov_chain"&gt;Markov chains&lt;/a&gt;. Also known as Markov models, these are processes that let us use statistics collected about an input to probabilistically generate an output.&lt;/p&gt;

&lt;p&gt;You can create a Markov chain by going through a body of text (&amp;quot;training corpus&amp;quot;) and counting how many times each word occurs, and for each word, what words follow it and how often. You create a profile of the text: how often can you expect a particular word to come up, and what other words you expect to follow it? You end up with a formula for creating utterances &amp;quot;in the style&amp;quot; of your training corpus.&lt;/p&gt;

&lt;p&gt;To use subreddits as a training corpus, we ran the following SQL query in BigQuery against the aforementioned Reddit comment tables:&lt;/p&gt;
&lt;pre class="code sql"&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nextword&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;c&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;LEAD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;OVER&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;nextword&lt;/span&gt;
  &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
      &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;pos&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
      &lt;span class="n"&gt;FLATTEN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="k"&gt;SELECT&lt;/span&gt;
          &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="k"&gt;POSITION&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;pos&lt;/span&gt;
        &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
          &lt;span class="k"&gt;SELECT&lt;/span&gt;
            &lt;span class="n"&gt;SPLIT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;REGEXP_REPLACE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;LOWER&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'(&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;
          &lt;span class="k"&gt;FROM&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;reddit_comments&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="n"&gt;_02&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;reddit_comments&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="n"&gt;_01&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;reddit_comments&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2016&lt;/span&gt;&lt;span class="n"&gt;_12&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;subreddit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'dankmemes'&lt;/span&gt; &lt;span class="k"&gt;LIMIT&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
         &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
         &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="k"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="k"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nextword&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;nextword&lt;/span&gt;
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
  &lt;span class="k"&gt;c&lt;/span&gt; &lt;span class="k"&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a modified version of the code provided by &lt;a href="https://twitter.com/felipehoffa"&gt;Felipe Hoffa&lt;/a&gt; on &lt;a href="http://blog.gdeltproject.org/making-ngrams-bigquery-scale/"&gt;the GDELT blog&lt;/a&gt;. It reads in the specified tables (&lt;code&gt;[fh-bigquery:reddit_comments.2017_02]&lt;/code&gt;, etc.) and creates a new table of words, &amp;#39;nextword&amp;#39;s, and the count (&amp;#39;c&amp;#39;) of that combination.&lt;/p&gt;

&lt;p&gt;Training corpora were created by swapping out the subreddit we looked for -- &lt;code&gt;WHERE subreddit = &amp;#39;dankmemes&amp;#39;&lt;/code&gt; makes our query only count words in comments that had &lt;code&gt;dankmemes&lt;/code&gt; listed as a subreddit. We cleaned up the data a bit to make processing easier -- words were all lowercased so we wouldn&amp;#39;t have to deal with capitalization, and we stripped out line breaks.&lt;/p&gt;

&lt;p&gt;We created nine training corpora this way, based on output tables ranging in size from 513KB to 7.16MB. Tolstoy&amp;#39;s 864-page &lt;em&gt;Anna Karenina&lt;/em&gt;, for comparison, takes up about 1.9MB. We had plenty of data; we were ready to make some bots.&lt;/p&gt;
&lt;h2 class='section-title' id=back-to-the-frontend&gt;&lt;a href='#back-to-the-frontend' class='section-inner'&gt;Back to the Frontend&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While I was getting the data and reworking it into a usable form, Alec and Steve were putting together our project&amp;#39;s frontend. We had decided that we wanted to make something that looked like a chat app, and have our bots appear to text each other as they conversed.&lt;/p&gt;

&lt;p&gt;Since the bulk of my work has been web-based, even my installations have leaned heavily on Google Chrome as a frontend. Alec, however, is most experienced developing with &lt;a href="https://unity3d.com/"&gt;Unity&lt;/a&gt;, a game engine used widely for 3D games and Virtual Reality experiences. I associate Unity so tightly with those types of projects that I was astounded when Alec volunteered to make our 2D frontend in Unity. It turns out that Unity can also be a powerful way to make UIs and other 2D products, and that Alec knew a Unity plugin that would let us create a connection between his Unity frontend and my &lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; backend with WebSockets.&lt;/p&gt;

&lt;p&gt;Alec and Steve teamed up on creating a frontend that mimicked an Android texting UI, complete with user avatars pulled from the subreddits&amp;#39; header images. We would randomly choose two bots and have them &amp;quot;converse&amp;quot; with each other. The conversation would go on for around twenty utterances per bot; then we would choose two new bots and start again.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/ahd/test1.jpg" alt="A photo of a laptop screen showing our project. A mocked Android texting UI shows language generated by our bots." title="One of our first tests." width="800px" height="1067px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;It&amp;#39;s alive! One of our first tests.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;The frontend sent, via WebSockets, an event to the backend requesting an utterance from a specific bot. The backend would generate an utterance on the spot and send that text back. The frontend controlled the conversation flow; it would wait to request a new utterance based on how long the previous utterance would take to read.&lt;/p&gt;

&lt;p&gt;We set up a router to let our laptops talk to each other via local IPs (the building&amp;#39;s WiFi wasn&amp;#39;t cooperating), and voil&amp;agrave; -- we were able to run our first tests. The actual installation would have the server and frontend on the same computer; we wouldn&amp;#39;t even need an internet connection.&lt;/p&gt;
&lt;h2 class='section-title' id=making-bots-chat&gt;&lt;a href='#making-bots-chat' class='section-inner'&gt;Making Bots Chat&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The last stage of the project was cleanup -- on the frontend, finessing assets and behavior; on the backend, stripping out badly-encoded characters (which appeared as &amp;#65533;) and trying to get the bots to talk to each other.&lt;/p&gt;

&lt;p&gt;So far we had been generating utterances apropos of nothing, probabilistically picking a starting word from a bot&amp;#39;s corpus and building the rest of the utterance from there. Our goal, though, was to make the bots&amp;#39; texting feel like a conversation, which meant relating their utterances somehow. One way to do this is to relate utterances to predetermined &amp;quot;topics&amp;quot; and pick appropriate responses filed under that same topic. Steve had looked into achieving this with &lt;a href="http://www.alicebot.org/aiml.html"&gt;AIML&lt;/a&gt;, a markup language for chatbots, earlier on in the weekend. That approach, however, required a smaller, more manually-determined set of utterances than what we had on hand.&lt;/p&gt;

&lt;p&gt;We decided it would be enough to get the bots to use the same word across utterances, even if we couldn&amp;#39;t guarantee a match between their utterances&amp;#39; actual themes.&lt;/p&gt;

&lt;p&gt;We started tracking every utterance in the conversation, ranking the words in an utterance from least- to most-used, assuming that the most common words (&amp;quot;a&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;I&amp;quot;) are the least important to establishing a theme and that more unique words (&amp;quot;Iraq&amp;quot;, &amp;quot;congress&amp;quot;, &amp;quot;emails&amp;quot;) are more indicative of an utterance&amp;#39;s theme. When generating a new utterance, we would pick its starting word by trying to find one of the more &amp;quot;interesting&amp;quot; words from the previous utterance in the current bot&amp;#39;s corpus. Some words were only known by certain bots, but more often than not, the desired word was known by both bots, albeit with different connotations.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a sample exchange between two bots that demonstrates &amp;quot;staying on topic&amp;quot;:&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;&lt;strong&gt;dankmemes&lt;/strong&gt;: rights die.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The_Donald&lt;/strong&gt;: die.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;dankmemes&lt;/strong&gt;: what yeah, dank [deleted] dank great?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The_Donald&lt;/strong&gt;: yeah, oh the third was woman drones. is officials.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;dankmemes&lt;/strong&gt;: yeah, you bot, eating questions re-submit harder being ^^/r/ayylmao2dongerbot thrusting regret a the in actually methinks. basking that get brethren, this more to words.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The_Donald&lt;/strong&gt;: yeah, oh so.. don&amp;#39;t hate what that wtf!!!&lt;/p&gt;

        &lt;cite&gt;
          
        &lt;/cite&gt;
      &lt;/blockquote&gt;
    &lt;h2 class='section-title' id=opening-night&gt;&lt;a href='#opening-night' class='section-inner'&gt;Opening Night&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;AHD culminated in an art show taking over the ground floor and mezzanine of PNCA. Every group installed its project and set it up to be on display for several hours that night.&lt;/p&gt;

&lt;p&gt;&lt;div class="image-wrap image-primary"&gt;
    &lt;img src="/images/ahd/show1.jpg" alt="A photo taken from overhead showing groups of people clustered around various artworks displayed on small white tables." title="The AHD exhibition at PNCA." width="800px" height="600px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;The diversity of skills brought into AHD meant that a huge variety of projects came out of it. There were Kinect and HoloLens projects; there was 16mm film and a dating app for animated monsters; there was a DDR dance pad that made emoji rain across a VR scene.&lt;/p&gt;

&lt;p&gt;Our project was surprisingly easy to set up and ran beautifully all evening. We ran an exported Unity app on a Mac Mini and projected it into a fake smartphone cut out of foamcore.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-primary"&gt;
    &lt;img src="/images/ahd/group.jpg" alt="A photo of three people, two men and a woman. They stand next to a projector, in front of a false wall onto which is projected a mimicked texting exchange." title="My group poses with our chatbots." width="800px" height="600px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Live at PNCA! From left to right: Steve, Alec, and me.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Visitors ebbed and flowed through the gallery. Since our project wasn&amp;#39;t interactive, I had been afraid that people might not feel interested or engaged, but we ended up with a piece that still drew visitors in and held their attention. Part of the interest, I think, came from being able to see a very topical, polemic bot (like &lt;code&gt;/r/The_Donald&lt;/code&gt;), or to see bots of opposing views (&lt;code&gt;/r/Feminism&lt;/code&gt; and &lt;code&gt;/r/TheRedPill&lt;/code&gt;, for example) matched against each other.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-primary"&gt;
    &lt;img src="/images/ahd/show2.jpg" alt="A photo taken from overhead shows groups of people looking at art. One group is clustered around a projector; one person points at the projection." title="Our art at the opening show." width="800px" height="600px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;people ‚ù§Ô∏è looking ‚ù§Ô∏è at ‚ù§Ô∏è our ‚ù§Ô∏è art&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;I loved working on this project and am looking for ways to keep working with it. Maybe as a foray into machine learning?&lt;/p&gt;

&lt;p&gt;Thanks to Alec and Steve for being amazing teammates and setting us all up for success on a super fun, challenging project. I look forward to lots more art-hacking in the future.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;This was my first blog post that wasn&amp;#39;t about making this blog! Stay tuned for a post about Markov chains.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
</feed>
